{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390ab208-b078-4cad-a704-a66da071182d",
   "metadata": {},
   "source": [
    "# 1.2、深度学习入门修炼手册：基于PyTorch的神经网络\n",
    "\n",
    "## 一、基于PyTorch的单层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afb1a7-231f-4d3c-a764-39a8e335c6a2",
   "metadata": {},
   "source": [
    "### 1.1 搭建线性层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8753fcb-ca34-4e6c-904c-ea14d5c52cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重矩阵的shape:  torch.Size([1024, 256])\n",
      "偏置矩阵的shape:  torch.Size([1024])\n",
      "torch.Size([7, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 生成一批数据\n",
    "B, C_in, C_out = 7, 256, 1024\n",
    "x = torch.randn(B, C_in)\n",
    "\n",
    "# 定义一层线性层\n",
    "linear_layer = nn.Linear(in_features=C_in, out_features=C_out, bias=True)\n",
    "\n",
    "# 打印线性层的权重矩阵的shape，即数据的维度\n",
    "print(\"权重矩阵的shape: \", linear_layer.weight.shape)\n",
    "\n",
    "# 打印线性层的偏置向量的shape\n",
    "print(\"偏置矩阵的shape: \", linear_layer.bias.shape)\n",
    "\n",
    "# 线性映射\n",
    "z = linear_layer(x)\n",
    "\n",
    "# 打印线性输出的shape\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2884be8b-2f87-40ea-9a83-89c1cb86d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer.weight.requires_grad)\n",
    "print(linear_layer.bias.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a55ca-40ee-4e6a-b68d-cf868280d957",
   "metadata": {},
   "source": [
    "### 1.2 搭建非线性激活函数\n",
    "\n",
    "随后，我们在线性层之后接一层非线性激活函数，这里使用到PyTorch框架提供的 `nn.Sigmoid` 类来定义 `sigmoid` 激活函数层，代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92da3322-d141-4c0d-a4ab-6a3c0b1a07ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1024])\n"
     ]
    }
   ],
   "source": [
    "# 定义Sigmoid激活函数\n",
    "activation = nn.Sigmoid()\n",
    "\n",
    "# 非线性激活输出\n",
    "y = activation(z)\n",
    "\n",
    "# 打印非线性输出的shape\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e7d12-3797-4f5e-82f0-aa8464492cce",
   "metadata": {},
   "source": [
    "另外，除了这种定义的方式，我们还可以使用torch.nn.functional.sigmoid函数来实现同样的sigmoid函数功能，如下所示，我们将两种方法的结果做减法，然后使用sum操作求和，如果输出是0，表明两种方法是等价的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6e4fba-0a69-484a-9487-38f75cd567c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y2 = F.sigmoid(z)\n",
    "diff = y - y2\n",
    "print(diff.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd256c-af50-4cac-8096-095bade123dc",
   "metadata": {},
   "source": [
    "### 1.3 搭建单层感知机\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf2ddf0-2a47-420d-b940-308cbb88e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准的PyTorch规范的神经网络模型搭建\n",
    "class SingleLayerPerceptron(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(in_features=in_dim, out_features=out_dim, bias=True)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.layer(x)\n",
    "        y = self.act(z)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6607dfeb-9cbe-446c-8842-cb8a73651d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1024])\n"
     ]
    }
   ],
   "source": [
    "# 基于上方的单层感知机类，来构建一个模型\n",
    "model = SingleLayerPerceptron(in_dim=256, out_dim=1024)\n",
    "\n",
    "# 模型前向推理\n",
    "y = model(x)\n",
    "\n",
    "# 查看输出的shape\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fd805-e58a-43a8-8433-de52de27b89d",
   "metadata": {},
   "source": [
    "## 二、基于PyTorch的多层感知机\n",
    "\n",
    "### 2.1 搭建多层感知机\n",
    "\n",
    "下方的代码给出了一个简单的三层感知机的PyTorch代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f2112da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, in_dim, inter_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            # 第一层感知机\n",
    "            nn.Linear(in_features=in_dim, out_features=inter_dim, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "            # 第二层感知机\n",
    "            nn.Linear(in_features=inter_dim, out_features=inter_dim, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "            # 第三层感知机\n",
    "            nn.Linear(in_features=inter_dim, out_features=out_dim, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layer(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c851813",
   "metadata": {},
   "source": [
    "在上方的代码中，我们用到了nn.Sequential类， 该类的作用是可以把传进内部的神经网络层串联在一起，我们只需要传进去一个数据数据，它会自行将输入数据从第一层一直传到最后一层，然后输出，注意，为了确保该类能正常运行，需要保证内部的层中，前一层的输出接口能有效对上后一层的输入接口，否则，会出现报错。比如，如果第一层需要需要处理输入的x，输出y，而第二层需要处理输入的x1和x2，但由于第一层只有一个y输出来，并流进第二层，那么第二层就会缺少一个有效的输入，便会导致报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27846c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1024])\n"
     ]
    }
   ],
   "source": [
    "# 基于上方的多层感知机类，来构建一个模型\n",
    "model = MultiLayerPerceptron(in_dim=C_in, inter_dim=2048, out_dim=C_out)\n",
    "\n",
    "# 模型前向推理\n",
    "y = model(x)\n",
    "\n",
    "# 打印输出的shape\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca4cf6",
   "metadata": {},
   "source": [
    "### 2.2 计算训练损失\n",
    "\n",
    "搭建玩感知机模型，了解了前向推理的代码，接下来便可以去计算模型的损失，以便我们后续去训练这个模型。我们采用上方的多层感知机模型，并随机生成一组数据，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba33353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[0.5079],\n",
      "        [0.5075],\n",
      "        [0.5135]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 生成一批数据\n",
    "B, C_in, C_out = 3, 256, 1\n",
    "x = torch.randn(B, C_in)\n",
    "\n",
    "# 基于上方的多层感知机类，来构建一个模型\n",
    "model = MultiLayerPerceptron(in_dim=C_in, inter_dim=2048, out_dim=C_out)\n",
    "\n",
    "# 模型前向推理\n",
    "y = model(x)\n",
    "\n",
    "# 打印输出的shape\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49269ca7",
   "metadata": {},
   "source": [
    "在上方的代码中，我们省略了多层感知机的代码，以节省篇幅。我们定义了一组新的数据，其中数据的数量 $B$ 是 3，输入数据的特征维度 $C_{in}$ 是256，感知机的最终输出维度 $C_{out}$ 是 1，因为我们这里要模拟一个针对二分类任务的损失计算。\n",
    "\n",
    "运行上方代码后，我们会得到输出 $y$ 的shape是形如[B, 1] 的。然后，我们随机定义了一组仅包含0和1的标签，其 `shape` 与模型输出的 `shape` 保持一致，接着，调用 PyTorch 提供的 `nn.BCELoss` 类来定义用于计算二元交叉熵的 `criterion` 变量 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "110c54e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签:  tensor([[0.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "二元交叉熵损失： tensor([[0.7090],\n",
      "        [0.6783],\n",
      "        [0.7205]], grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "二元交叉熵损失的shape： torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 定义标签\n",
    "target = torch.randint(low=0, high=2, size=[B, 1]).float()\n",
    "print(\"标签: \", target)\n",
    "\n",
    "# 定义二元交叉熵损失函数\n",
    "criterion = nn.BCELoss(reduction=\"none\")\n",
    "\n",
    "# 计算二元交叉熵损失\n",
    "loss = criterion(y, target)\n",
    "print(\"二元交叉熵损失：\", loss)\n",
    "print(\"二元交叉熵损失的shape：\", loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39edac1f",
   "metadata": {},
   "source": [
    "在上方代码中，nn.BCELoss类中的reduction参数被设置为none，表明我们不需要该来在计算完后对输出做任何处理。如果我们需要该类会把所有样本的损失加在一起，可以将reduction设置为sum；如果需要将所有样本的损失做个平均，则设置为mean，感兴趣的读者可以自行调试。\n",
    "\n",
    "我们运行代码，即可看到计算出来的二元交叉熵损失和shape，下方给出了笔者的输出示例，由于输入数据和标签都是随机生成的，读者的结果可能会与笔者的有些不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257fb99a",
   "metadata": {},
   "source": [
    "运行上方的代码，即可看到结果均为0的输出，如下所示，表明二者的计算过程是一致的。\n",
    "\n",
    "另外，在上方实现的多层感知机模型中，最后输出会被定义的sigmoid函数做处理，将其映射到0~1范围内，然后再去计算二元交叉熵，这是因为后者需要输入的数值在0~1的值域内。然而，由于sigmoid函数中的指数函数可能会造成某些计算的不稳定性，PyTorch额外提供了另一种计算二元交叉熵的nn.BCEWithLogitsLoss类和 binary_cross_entropy_with_logits函数来计算损失（二者也是等价的），对于这两个方法，我们就不需要在外部做sigmoid激活操作，内部会以等价的、但更稳定的计算方式来处理，并计算损失。\n",
    "\n",
    "为了测试两种计算二元交叉熵的计算方法，我们编写了如下所示的代码，其中，criterion便是我们此前定义的nn.BCELoss类，要求输入的预测已被sigmoid函数处理过，criterion2则是 nn.BCEWithLogitsLoss类，无需我们外部对预测值进行Sigmoid操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886de007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1269, 3.0486, 0.0181])\n",
      "tensor([2.1269, 3.0486, 0.0181])\n",
      "tensor([2.1269, 3.0486, 0.0181])\n"
     ]
    }
   ],
   "source": [
    "criterion2 = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "y = torch.tensor([-2, 3, 4]).float()\n",
    "target = torch.tensor([1, 0, 1]).float()\n",
    "loss1 = criterion(F.sigmoid(y), target)\n",
    "print(loss1)\n",
    "loss2 = criterion2(y, target)\n",
    "print(loss2)\n",
    "loss3 = F.binary_cross_entropy_with_logits(y, target, reduction=\"none\")\n",
    "print(loss3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88608b8f",
   "metadata": {},
   "source": [
    "运行上方的代码，我们即可看到完全相等的输出，如下所示，表明三者的操作完全是等价。\n",
    "\n",
    "因此，通常情况下， 我们往往会使用nn.BCEWithLogitsLoss类来定义二元交叉熵函数，避免运算的过程中出现恼人的NAN问题，当然，在有些时候，我们也会用nn.BCELoss类来处理已经被映射到0~1范围内的数值，二者并不是一个完全A取代B的关系，需要我们自己根据具体情况来做具体的选择。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
