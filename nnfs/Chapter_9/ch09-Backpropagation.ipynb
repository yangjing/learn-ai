{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "**反向传播**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单一梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.44, -0.38, -0.07,  1.37]),\n",
       " array([ 0.44, -0.38, -0.07,  1.37]),\n",
       " array([ 0.44, -0.38, -0.07,  1.37], dtype=float32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvalues = np.array([[1.0, 1.0, 1.0]])\n",
    "\n",
    "# fmt: off\n",
    "# 有 3 组权重，每个神经元一组；有 4 个输入，因此为 4 个权重\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "# fmt: on\n",
    "\n",
    "dx0 = sum(weights[0] * dvalues[0])\n",
    "dx1 = sum(weights[1] * dvalues[0])\n",
    "dx2 = sum(weights[2] * dvalues[0])\n",
    "dx3 = sum(weights[3] * dvalues[0])\n",
    "\n",
    "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
    "\n",
    "# weights 与 dvalues 相乘时，dvalues 将自动按行广播来匹配到 weights(4, 3)  shape dvalues(3, 4)\n",
    "dinputs2 = np.sum(weights * dvalues, axis=1)\n",
    "\n",
    "dinputs3 = np.dot(dvalues[0], weights.T)\n",
    "\n",
    "\n",
    "dinputs, dinputs2, dinputs3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dinputs =\n",
      " [[ 0.42  0.42  0.42]\n",
      " [-1.83 -1.83 -1.83]\n",
      " [ 0.53  0.53  0.53]\n",
      " [ 2.61  2.61  2.61]]\n",
      "dweights =\n",
      " [[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n",
      "dbiases =\n",
      " [[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "dvalues = np.array([[1.0, 1.0, 1.0],\n",
    "                    [2.0, 2.0, 2.0],\n",
    "                    [3.0, 3.0, 3.0]])\n",
    "# 有 3 组权重，每个神经元一组；有 4 个输入，因此为 4 个权重\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]])\n",
    "# fmt: on\n",
    "\n",
    "dinputs = np.dot(weights.T, dvalues)  # np.dot(dvalues, weights)\n",
    "print(\"dinputs =\\n\", dinputs)\n",
    "\n",
    "# fmt: off\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                   [2.0, 5.0, -1.0, 2.0],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]])\n",
    "# fmt: on\n",
    "\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "print(\"dweights =\\n\", dweights)\n",
    "\n",
    "# 每个神经元一个偏置\n",
    "biases = np.array([2, 3, 0.5])\n",
    "# 对值求和，在样本（第 1 个轴）上进行，保持维度\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "print(\"dbiases =\\n\", dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "  # Layer initialization\n",
    "  def __init__(self, n_inputs, n_neurons):\n",
    "    # Initialize weights and biases\n",
    "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "    self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "  # Forward pass\n",
    "  def forward(self, inputs):\n",
    "    # Remember input values\n",
    "    self.inputs = inputs\n",
    "    # Calculate output values from inputs, weights and biases\n",
    "    self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "  # Backward pass\n",
    "  def backward(self, dvalues):\n",
    "    # Gradients on parameters\n",
    "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "    # Gradient on values\n",
    "    self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "  # Forward pass\n",
    "  def forward(self, inputs):\n",
    "    # Remember input values\n",
    "    self.inputs = inputs\n",
    "    # Calculate output values from inputs\n",
    "    self.output = np.maximum(0, inputs)\n",
    "\n",
    "  # Backward pass\n",
    "  def backward(self, dvalues):\n",
    "    # Since we need to modify original variable,\n",
    "    # let's make a copy of values first\n",
    "    self.dinputs = dvalues.copy()\n",
    "\n",
    "    # Zero gradient where input values were negative\n",
    "    self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "  # Forward pass\n",
    "  def forward(self, inputs):\n",
    "    # Remember input values\n",
    "    self.inputs = inputs\n",
    "\n",
    "    # Get unnormalized probabilities\n",
    "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "    # Normalize them for each sample\n",
    "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "    self.output = probabilities\n",
    "\n",
    "  # Backward pass\n",
    "  def backward(self, dvalues):\n",
    "    # Create uninitialized array\n",
    "    self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "    # Enumerate outputs and gradients\n",
    "    for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "      # Flatten output array\n",
    "      single_output = single_output.reshape(-1, 1)\n",
    "      # Calculate Jacobian matrix of the output\n",
    "      # S_{i,j}KroneckerDelta_{i,j} - S_{i,j}S_{i,k}\n",
    "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "      # Calculate sample-wise gradient\n",
    "      # and add it to the array of sample gradients\n",
    "      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "  # Calculates the data and regularization losses\n",
    "  # given model output and ground truth values\n",
    "  def calculate(self, output, y):\n",
    "    # Calculate sample losses\n",
    "    sample_losses = self.forward(output, y)\n",
    "\n",
    "    # Calculate mean loss\n",
    "    data_loss = np.mean(sample_losses)\n",
    "\n",
    "    # Return loss\n",
    "    return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "  # Forward pass\n",
    "  def forward(self, y_pred, y_true):\n",
    "    # Number of samples in a batch\n",
    "    samples = len(y_pred)\n",
    "\n",
    "    # Clip data to prevent division by 0\n",
    "    # Clip both sides to not drag mean towards any value\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "    # Probabilities for target values -\n",
    "    # only if categorical labels\n",
    "    if len(y_true.shape) == 1:\n",
    "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "    # Mask values - only for one-hot encoded labels\n",
    "    elif len(y_true.shape) == 2:\n",
    "      correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "    # Losses\n",
    "    negative_log_likelihoods = -np.log(correct_confidences)\n",
    "    return negative_log_likelihoods\n",
    "\n",
    "  # Backward pass\n",
    "  def backward(self, dvalues, y_true):\n",
    "    # Number of samples\n",
    "    samples = len(dvalues)\n",
    "    # Number of labels in every sample\n",
    "    # We'll use the first sample to count them\n",
    "    labels = len(dvalues[0])\n",
    "\n",
    "    # If labels are sparse, turn them into one-hot vector\n",
    "    if len(y_true.shape) == 1:\n",
    "      y_true = np.eye(labels)[y_true]\n",
    "\n",
    "    # np.mean(-y_true / dvalues)\n",
    "    # Calculate gradient\n",
    "    self.dinputs = -y_true / dvalues\n",
    "    # Normalize gradient\n",
    "    self.dinputs = self.dinputs / samples  # np.mean(-y_true / dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "  # Creates activation and loss function objects\n",
    "  def __init__(self):\n",
    "    self.activation = Activation_Softmax()\n",
    "    self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "  # Forward pass\n",
    "  def forward(self, inputs, y_true):\n",
    "    # Output layer's activation function\n",
    "    self.activation.forward(inputs)\n",
    "    # Set the output\n",
    "    self.output = self.activation.output\n",
    "    # Calculate and return loss value\n",
    "    return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "  # Backward pass\n",
    "  def backward(self, dvalues, y_true):\n",
    "    # Number of samples\n",
    "    samples = len(dvalues)\n",
    "\n",
    "    # If labels are one-hot encoded,\n",
    "    # turn them into discrete values\n",
    "    if len(y_true.shape) == 2:\n",
    "      y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "    # Copy so we can safely modify\n",
    "    self.dinputs = dvalues.copy()\n",
    "    # Calculate gradient\n",
    "    self.dinputs[range(samples), y_true] -= 1\n",
    "    # Normalize gradient\n",
    "    self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333335  0.3333335  0.33333305]\n",
      " [0.33333367 0.33333358 0.33333272]\n",
      " [0.33333355 0.33333385 0.33333263]\n",
      " [0.333333   0.33333418 0.33333278]]\n",
      "loss: 1.0986123\n",
      "acc: 0.32\n",
      "dense1.dweights:\n",
      " [[ 3.2762854e-04  2.5372763e-04 -3.7113682e-04]\n",
      " [-5.4703956e-05 -3.7952783e-04  2.2306606e-04]]\n",
      "dense1.dbiases:\n",
      " [[-0.00032174 -0.00043721  0.00053705]]\n",
      "dense2.dweights:\n",
      " [[-9.95226146e-05  3.27997783e-04 -2.28475197e-04]\n",
      " [ 1.01648984e-04 -2.43361646e-04  1.41712662e-04]\n",
      " [ 2.35507177e-04 -3.19360319e-04  8.38531341e-05]]\n",
      "dense2.dbiases:\n",
      " [[-7.707160e-06  5.553011e-07  6.938586e-06]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(39)\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "# Let's see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "\n",
    "# Print loss value\n",
    "print(\"loss:\", loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "  y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"acc:\", accuracy)\n",
    "\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# Print gradients\n",
    "print(\"dense1.dweights:\\n\", dense1.dweights)\n",
    "print(\"dense1.dbiases:\\n\", dense1.dbiases)\n",
    "print(\"dense2.dweights:\\n\", dense2.dweights)\n",
    "print(\"dense2.dbiases:\\n\", dense2.dbiases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较两种算法的速度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07109954100451432 0.2592502910119947\n",
      "3.646300487306017\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "\n",
    "def f1():\n",
    "  softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "  softmax_loss.backward(softmax_outputs, class_targets)\n",
    "  dvalues1 = softmax_loss.dinputs\n",
    "\n",
    "\n",
    "def f2():\n",
    "  activation = Activation_Softmax()\n",
    "  activation.output = softmax_outputs\n",
    "  loss = Loss_CategoricalCrossentropy()\n",
    "  loss.backward(softmax_outputs, class_targets)\n",
    "  activation.backward(loss.dinputs)\n",
    "  dvalues2 = activation.dinputs\n",
    "\n",
    "\n",
    "t1 = timeit(lambda: f1(), number=10000)\n",
    "t2 = timeit(lambda: f2(), number=10000)\n",
    "print(t1, t2)\n",
    "print(t2 / t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
