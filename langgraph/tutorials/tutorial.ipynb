{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f007212e",
   "metadata": {},
   "source": [
    "# LangGraph Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc95951",
   "metadata": {},
   "source": [
    "## Import Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda454b",
   "metadata": {},
   "source": [
    "1. å¯¼å…¥ç›¸å…³çš„åº“ã€æ¨¡å—\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bdbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845fc5f",
   "metadata": {},
   "source": [
    "### å®šä¹‰ LangGraph çŠ¶æ€å›¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "  messages: list[AnyMessage]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "# from langchain.chat_models import init_chat_model\n",
    "# llm = init_chat_model('deepseek:deepseek-chat')\n",
    "llm = ChatOpenAI(\n",
    "  model='Qwen/Qwen3-30B-A3B-Instruct-2507',\n",
    "  openai_api_key=os.environ['SILICONFLOW_API_KEY'],\n",
    "  openai_api_base='https://api.siliconflow.cn/v1',\n",
    "  streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93e392e",
   "metadata": {},
   "source": [
    "## Add tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f679577",
   "metadata": {},
   "source": [
    "å®šä¹‰å›¾å½¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3b3df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1233b3b00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "  return {'messages': [llm_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node('chatbot', chatbot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2041077c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'LangGraph çš„ `node` æ˜¯ä»€ä¹ˆï¼Ÿ',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'https://zhuanlan.zhihu.com/p/720864252',\n",
       "   'title': 'åˆå­¦LangGraph ä¹‹èŠ‚ç‚¹ã€è¾¹å’ŒçŠ¶æ€ - çŸ¥ä¹ä¸“æ ',\n",
       "   'content': 'èŠ‚ç‚¹ï¼ˆNodesï¼‰ï¼š æ˜¯å›¾çš„åŸºæœ¬æ„å»ºæ¨¡å— ã€‚ æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„åŠŸèƒ½æˆ–æ“ä½œï¼Œç”¨æ¥å¤„ç†å½“å‰çŠ¶æ€ã€‚ èŠ‚ç‚¹å¯ä»¥æ‰§è¡Œè®¡ç®—ã€ä¿®æ”¹çŠ¶æ€ï¼Œæˆ–è€…æ ¹æ®æ¥æ”¶åˆ°çš„è¾“å…¥ç”Ÿæˆè¾“å‡ºã€‚ ç®€å•ç†è§£å°±æ˜¯åœ¨èŠ‚ç‚¹å†…æ•°æ®çš„æµè½¬æ˜¯å›ºå®šçš„ã€‚',\n",
       "   'score': 0.8737439,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://zhuanlan.zhihu.com/p/681428515',\n",
       "   'title': 'LangChain å‘å¸ƒçš„ä¸€ä¸ªé‡è¦åŠŸèƒ½ï¼šLangGraph - çŸ¥ä¹ä¸“æ ',\n",
       "   'content': 'Node. å›¾ä¸­æœ€å…³é”®çš„å…ƒç´ ä¹‹ä¸€æ˜¯èŠ‚ç‚¹ã€‚æ¯ä¸ªLangGraphèŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªåç§°å’Œå…¶å€¼ï¼Œå®ƒå¯ä»¥æ˜¯LCELä¸­çš„å‡½æ•°æˆ–å¯è¿è¡Œé¡¹ã€‚æ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶ä¸€ä¸ªå­—å…¸ç±»å‹çš„æ•°æ®ï¼Œå…¶ç»“æ„',\n",
       "   'score': 0.8687491,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æµ‹è¯•å·¥å…·\n",
    "tool.invoke('LangGraph çš„ `node` æ˜¯ä»€ä¹ˆï¼Ÿ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b5852c",
   "metadata": {},
   "source": [
    "åˆ›å»ºè‡ªå®šä¹‰ Tool å‡½æ•°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d4c3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1233b3b00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "  \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "  def __init__(self, tools: list) -> None:\n",
    "    self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "  def __call__(self, inputs: dict):\n",
    "    if messages := inputs.get('messages', []):\n",
    "      message = messages[-1]\n",
    "    else:\n",
    "      raise ValueError('No message found in input')\n",
    "    outputs = []\n",
    "    for tool_call in message.tool_calls:\n",
    "      print(f'Call tool function: {tool_call[\"name\"]}, args: {tool_call[\"args\"]}')\n",
    "      tool_result = self.tools_by_name[tool_call['name']].invoke(tool_call['args'])\n",
    "      outputs.append(\n",
    "        ToolMessage(\n",
    "          content=json.dumps(tool_result),\n",
    "          name=tool_call['name'],\n",
    "          tool_call_id=tool_call['id'],\n",
    "        )\n",
    "      )\n",
    "    return {'messages': outputs}\n",
    "\n",
    "\n",
    "# å®šä¹‰å·¥å…·å‡½æ•°èŠ‚ç‚¹\n",
    "tool_node = BasicToolNode(tools=[tool])\n",
    "graph_builder.add_node('tools', tool_node)\n",
    "\n",
    "\n",
    "# è·¯ç”±å‡½æ•°å®ç°\n",
    "def route_tools(state: State):\n",
    "  \"\"\"\n",
    "  Use in the conditional_edge to route to the ToolNode if the last message\n",
    "  has tool calls. Otherwise, route to the end.\n",
    "  \"\"\"\n",
    "  if isinstance(state, list):\n",
    "    ai_message = state[-1]\n",
    "  elif messages := state.get('messages', []):\n",
    "    ai_message = messages[-1]\n",
    "  else:\n",
    "    raise ValueError(f'No messages found in input state to tool_edge: {state}')\n",
    "  if hasattr(ai_message, 'tool_calls') and len(ai_message.tool_calls) > 0:\n",
    "    return 'tools'\n",
    "  return END\n",
    "\n",
    "\n",
    "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "  'chatbot',\n",
    "  route_tools,\n",
    "  # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
    "  # It defaults to the identity function, but if you\n",
    "  # want to use a node named something else apart from \"tools\",\n",
    "  # You can update the value of the dictionary to something else\n",
    "  # e.g., \"tools\": \"my_tools\"\n",
    "  {\n",
    "    'tools': 'tools',\n",
    "    END: END,\n",
    "  },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665876e",
   "metadata": {},
   "source": [
    "ä½¿ç”¨é¢„å®šä¹‰çš„å·¥å…·å‡½æ•°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93f2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# tool_node = ToolNode(tools=[tool])\n",
    "# graph_builder.add_node('tools', tool_node)\n",
    "\n",
    "# graph_builder.add_conditional_edges(\n",
    "#   'chatbot',\n",
    "#   tools_condition,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eca857",
   "metadata": {},
   "source": [
    "## Add memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0505166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "memory = InMemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c2d3dd",
   "metadata": {},
   "source": [
    "æ„å»ºå¹¶ç¼–è¯‘å›¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9226da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge('tools', 'chatbot')\n",
    "graph_builder.add_edge(START, 'chatbot')\n",
    "graph = graph_builder.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e176953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630421e",
   "metadata": {},
   "source": [
    "### é€‰æ‹©ä¸€ä¸ªä¸»é¢˜ï¼ˆthread idï¼‰ä½œä¸ºæœ¬æ¬¡å¯¹è¯çš„å…³é”®è¯ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc872690",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'configurable': {'thread_id': '1'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b37571c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stream_graph_updates(user_input: str):\n",
    "#   for event in graph.stream(\n",
    "#     {'messages': [{'role': 'user', 'content': user_input}]},\n",
    "#     config,\n",
    "#     stream_mode='values',\n",
    "#   ):\n",
    "#     for value in event.values():\n",
    "#       print('Assistant:', value['messages'][-1].content)\n",
    "\n",
    "\n",
    "# while True:\n",
    "#   try:\n",
    "#     user_input = input('User: ')\n",
    "#     if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "#       print('Goodbye!')\n",
    "#       break\n",
    "\n",
    "#     stream_graph_updates(user_input)\n",
    "#   except Exception:\n",
    "#     # fallback if input() is not available\n",
    "#     user_input = 'What do you know about LangGraph?'\n",
    "#     print('User: ' + user_input)\n",
    "#     stream_graph_updates(user_input)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b232c4a0",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ thread id: 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b874b7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "ä½ å¥½ï¼æˆ‘çš„åå­—å« æ¨æ™¯\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼Œæ¨æ™¯ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "user_input = 'ä½ å¥½ï¼æˆ‘çš„åå­—å« æ¨æ™¯'\n",
    "\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "  {'messages': [{'role': 'user', 'content': user_input}]},\n",
    "  config,\n",
    "  stream_mode='values',\n",
    ")\n",
    "for event in events:\n",
    "  event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a77637fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å½“ç„¶è®°å¾—ï¼Œä½ çš„åå­—æ˜¯æ¨æ™¯ï¼ğŸ˜Š æœ‰ä»€ä¹ˆé—®é¢˜æˆ–è¯é¢˜æƒ³å’Œæˆ‘èŠèŠå—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "user_input = 'è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ'\n",
    "\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "  {'messages': [{'role': 'user', 'content': user_input}]},\n",
    "  config,\n",
    "  stream_mode='values',\n",
    ")\n",
    "for event in events:\n",
    "  event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b448bed",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ thread id: 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58d04511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "æˆ‘æ— æ³•è®°ä½ç”¨æˆ·çš„åå­—æˆ–ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œå› ä¸ºæ¯æ¬¡å¯¹è¯éƒ½æ˜¯ç‹¬ç«‹çš„ã€‚å¦‚æœæ‚¨æ„¿æ„ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘æ‚¨çš„åå­—ï¼Œæˆ‘ä¼šåœ¨æœ¬æ¬¡å¯¹è¯ä¸­ä½¿ç”¨å®ƒæ¥ç§°å‘¼æ‚¨ã€‚\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ä¸åŒçš„ thread_id è¿›è¡Œå¯¹è¯\n",
    "events = graph.stream(\n",
    "  {'messages': [{'role': 'user', 'content': user_input}]},\n",
    "  {'configurable': {'thread_id': '2'}},  # change the `thread_id` here to '2' instead of '1'\n",
    "  stream_mode='values',\n",
    ")\n",
    "for event in events:\n",
    "  event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6523a41f",
   "metadata": {},
   "source": [
    "### æ£€æŸ¥ state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "983c8d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='ä½ å¥½ï¼æˆ‘çš„åå­—å« æ¨æ™¯', additional_kwargs={}, response_metadata={}, id='bb453236-b335-4912-af74-46aa66c4184d'), AIMessage(content='ä½ å¥½ï¼Œæ¨æ™¯ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'Qwen/Qwen3-30B-A3B-Instruct-2507'}, id='run--d8ba66dd-0cb8-46cf-9748-078eedbc2f5d-0', usage_metadata={'input_tokens': 31025, 'output_tokens': 135, 'total_tokens': 31160, 'input_token_details': {}, 'output_token_details': {}}), HumanMessage(content='è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ', additional_kwargs={}, response_metadata={}, id='7ff4d74e-0f0c-47bd-a8aa-10182573161b'), AIMessage(content='å½“ç„¶è®°å¾—ï¼Œä½ çš„åå­—æ˜¯æ¨æ™¯ï¼ğŸ˜Š æœ‰ä»€ä¹ˆé—®é¢˜æˆ–è¯é¢˜æƒ³å’Œæˆ‘èŠèŠå—ï¼Ÿ', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'Qwen/Qwen3-30B-A3B-Instruct-2507'}, id='run--3649833e-7d98-4fdf-add5-ddfe7e3e97f9-0', usage_metadata={'input_tokens': 42665, 'output_tokens': 252, 'total_tokens': 42917, 'input_token_details': {}, 'output_token_details': {}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07656f-9ef0-682c-8004-7b19879487b0'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-08-11T02:00:38.094213+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07656f-8861-63fe-8003-c962908807c7'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-ai (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
